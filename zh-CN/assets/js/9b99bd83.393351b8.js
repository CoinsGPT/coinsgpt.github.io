"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([["17728"],{57234:function(e,n,t){t.d(n,{Z:()=>a});let a=t.p+"assets/images/bitcoin_data_pipeline-083bc34cccd93411924337896b88770f.png"},97005:function(e,n,t){t.d(n,{Z:()=>a});let a=t.p+"assets/images/kafka_01-807249e726cadc9d3be21375df967d42.png"},1470:function(e,n,t){t.r(n),t.d(n,{frontMatter:()=>r,default:()=>h,contentTitle:()=>o,assets:()=>l,toc:()=>c,metadata:()=>a});var a=JSON.parse('{"id":"bitcoin/kafka-table-engine","title":"05 Kafka Table Engine","description":"To use the Kafka table engine, you should be broadly familiar with ClickHouse materialized views.","source":"@site/docs/bitcoin/kafka-table-engine.md","sourceDirName":"bitcoin","slug":"/bitcoin/kafka-table-engine","permalink":"/zh-CN/docs/next/bitcoin/kafka-table-engine","draft":false,"unlisted":false,"editUrl":"https://crowdin.com/project/docusaurus-v2/zh-CN","tags":[],"version":"current","lastUpdatedBy":"thebestornothing","lastUpdatedAt":1748582758000,"frontMatter":{},"sidebar":"bitcoin","previous":{"title":"05 ClickHouse Schema Evolution","permalink":"/zh-CN/docs/next/bitcoin/clickhouse-schema-evolution"},"next":{"title":"06 Bitcoin Data Verification","permalink":"/zh-CN/docs/next/bitcoin/bitcoin-data-verfication"}}'),i=t(85893),s=t(80980);let r={},o="05 Kafka Table Engine",l={},c=[{value:"Overview",id:"overview",level:2},{value:"1. Prepare",id:"1-prepare",level:2},{value:"2. Create Database",id:"2-create-database",level:2},{value:"3. Create the destination table",id:"3-create-the-destination-table",level:2},{value:"4. Create and populate the topic",id:"4-create-and-populate-the-topic",level:2},{value:"5. Create the Kafka table engine",id:"5-create-the-kafka-table-engine",level:2},{value:"6. Create the materialized view",id:"6-create-the-materialized-view",level:2},{value:"7. Confirm rows have been inserted",id:"7-confirm-rows-have-been-inserted",level:5},{value:"Common Operations",id:"common-operations",level:4},{value:"Stopping &amp; restarting message consumption",id:"stopping--restarting-message-consumption",level:5},{value:"Adding Kafka Metadata",id:"adding-kafka-metadata",level:2},{value:"Modify Kafka Engine Settings",id:"modify-kafka-engine-settings",level:2},{value:"Debugging Issues",id:"debugging-issues",level:2}];function d(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h4:"h4",h5:"h5",header:"header",img:"img",p:"p",pre:"pre",strong:"strong",...(0,s.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"05-kafka-table-engine",children:"05 Kafka Table Engine"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:t(57234).Z+"",width:"1536",height:"1024"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-info",children:"https://clickhouse.com/docs/integrations/kafka/kafka-table-engine\n"})}),"\n",(0,i.jsx)(n.p,{children:"To use the Kafka table engine, you should be broadly familiar with ClickHouse materialized views."}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"Initially, we focus on the most common use case: using the Kafka table engine to insert data into ClickHouse from Kafka."}),"\n",(0,i.jsx)(n.p,{children:"The Kafka table engine allows ClickHouse to read from a Kafka topic directly. Whilst useful for viewing messages on a topic, the engine by design only permits one-time retrieval, i.e. when a query is issued to the table, it consumes data from the queue and increases the consumer offset before returning results to the caller. Data cannot, in effect, be re-read without resetting these offsets."}),"\n",(0,i.jsx)(n.p,{children:"To persist this data from a read of the table engine, we need a means of capturing the data and inserting it into another table. Trigger-based materialized views natively provide this functionality. A materialized view initiates a read on the table engine, receiving batches of documents. The TO clause determines the destination of the data - typically a table of the Merge Tree family. This process is visualized below:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:t(97005).Z+"",width:"2282",height:"1080"})}),"\n",(0,i.jsx)(n.h2,{id:"1-prepare",children:"1. Prepare"}),"\n",(0,i.jsxs)(n.p,{children:["If you have data populated on a target topic, you can adapt the following for use in your dataset. Alternatively, a sample Github dataset is provided ",(0,i.jsx)(n.a,{href:"https://datasets-documentation.s3.eu-west-3.amazonaws.com/kafka/github_all_columns.ndjson",children:"here"}),". This dataset is used in the examples below and uses a reduced schema and subset of the rows (specifically, we limit to Github events concerning the ",(0,i.jsx)(n.a,{href:"https://github.com/ClickHouse/ClickHouse",children:"ClickHouse repository"}),"), compared to the full dataset available ",(0,i.jsx)(n.a,{href:"https://ghe.clickhouse.tech/",children:"here"}),", for brevity. This is still sufficient for most of the queries ",(0,i.jsx)(n.a,{href:"https://ghe.clickhouse.tech/",children:"published with the dataset"})," to work."]}),"\n",(0,i.jsx)(n.h2,{id:"2-create-database",children:"2. Create Database"}),"\n",(0,i.jsxs)(n.p,{children:["We're also going to create a database called ",(0,i.jsx)(n.code,{children:"bitcoin"})," to use in this tutorial:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE DATABASE bitcoin;\n"})}),"\n",(0,i.jsx)(n.p,{children:"Once you've created the database, you'll need to switch over to it:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"USE bitcoin;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"3-create-the-destination-table",children:"3. Create the destination table"}),"\n",(0,i.jsx)(n.p,{children:"Prepare your destination table. In the example below we use the reduced GitHub schema for purposes of brevity. Note that although we use a MergeTree table engine, this example could easily be adapted for any member of the MergeTree family."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE blocks\n(\n  hash String,\n  size UInt64,\n  stripped_size UInt64,\n  weight UInt64,\n  number UInt64,\n  version UInt64,\n  merkle_root String,\n  timestamp DateTime,\n  timestamp_month Date,\n  nonce String,\n  bits String,\n  coinbase_param String,\n  transaction_count UInt64\n)\nENGINE = ReplacingMergeTree()\nPRIMARY KEY (hash)\nPARTITION BY toYYYYMM(timestamp_month)\nORDER BY hash;\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE transactions\n(\n  hash String,\n  size UInt64,\n  virtual_size UInt64,\n  version UInt64,\n  lock_time UInt64,\n  block_hash String,\n  block_number UInt64,\n  block_timestamp DateTime,\n  block_timestamp_month Date,\n  input_count UInt64,\n  output_count UInt64,\n  input_value Float64,\n  output_value Float64,\n  is_coinbase BOOL,\n  fee Float64,\n  inputs Array(Tuple(index UInt64, spent_transaction_hash String, spent_output_index UInt64, script_asm String, script_hex String, sequence UInt64, required_signatures UInt64, type String, addresses Array(String), value Float64)),\n  outputs Array(Tuple(index UInt64, script_asm String, script_hex String, required_signatures UInt64, type String, addresses Array(String), value Float64))\n)\nENGINE = ReplacingMergeTree()\nPRIMARY KEY (block_hash, hash)\nPARTITION BY toYYYYMM(block_timestamp_month)\nORDER BY (block_hash, hash);\n"})}),"\n",(0,i.jsx)(n.h2,{id:"4-create-and-populate-the-topic",children:"4. Create and populate the topic"}),"\n",(0,i.jsxs)(n.p,{children:["Next, we're going to create a topic. There are several tools that we can use to do this. If we're running Kafka locally on our machine or inside a Docker container, ",(0,i.jsx)(n.a,{href:"https://docs.redpanda.com/current/get-started/rpk-install/",children:"RPK"})," works well. We can create a topic called ",(0,i.jsx)(n.code,{children:"blocks"})," and ",(0,i.jsx)(n.code,{children:"transactions"})," with 3 partitions by running the following command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kafka-topics.sh \\\n  --create \\\n  --topic blocks \\\n  --bootstrap-server localhost:9092 \\\n  --partitions 3 \\\n  --replication-factor 1\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kafka-topics.sh \\\n  --create \\\n  --topic transactions \\\n  --bootstrap-server localhost:9092 \\\n  --partitions 3 \\\n  --replication-factor 1\n"})}),"\n",(0,i.jsx)(n.p,{children:"Now we need to add these topics to consumer group. We can run a command similar to the following if we're running Kafka locally with authentication disabled:"}),"\n",(0,i.jsxs)(n.p,{children:["You do ",(0,i.jsx)(n.strong,{children:"not"})," create consumer groups manually. A consumer group is ",(0,i.jsx)(n.strong,{children:"automatically created"})," the first time a consumer subscribes to a topic using a new ",(0,i.jsx)(n.code,{children:"--group"})," (or group.id in code)."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kafka-console-consumer.sh \\\n  --bootstrap-server localhost:9092 \\\n  --topic blocks \\\n  --group bitcoin-group\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kafka-console-consumer.sh \\\n  --bootstrap-server localhost:9092 \\\n  --topic transactions \\\n  --group bitcoin-group\n"})}),"\n",(0,i.jsxs)(n.p,{children:["This creates the ",(0,i.jsx)(n.code,{children:"bitcoin-group"})," consumer group if it doesn't already exist."]}),"\n",(0,i.jsx)(n.h2,{id:"5-create-the-kafka-table-engine",children:"5. Create the Kafka table engine"}),"\n",(0,i.jsxs)(n.p,{children:["The below example creates a table engine with the same schema as the merge tree table. This isn't strictly required, as you can have an alias or ephemeral columns in the target table. The settings are important; however - note the use of ",(0,i.jsx)(n.code,{children:"JSONEachRow"})," as the data type for consuming JSON from a Kafka topic. The values ",(0,i.jsx)(n.code,{children:"blocks"})," and ",(0,i.jsx)(n.code,{children:"bitcoin-group"})," represent the name of the topic and consumer group names, respectively. The topics can actually be a list of values."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE blocks_queue\n(\n  hash String,\n  size UInt64,\n  stripped_size UInt64,\n  weight UInt64,\n  number UInt64,\n  version UInt64,\n  merkle_root String,\n  timestamp DateTime,\n  timestamp_month Date,\n  nonce String,\n  bits String,\n  coinbase_param String,\n  transaction_count UInt64\n)\nENGINE = Kafka('localhost:9092', 'blocks', 'bitcoin-group', 'JSONEachRow') settings kafka_thread_per_consumer = 0, kafka_num_consumers = 3;\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE transactions_queue\n(\n  hash String,\n  size UInt64,\n  virtual_size UInt64,\n  version UInt64,\n  lock_time UInt64,\n  block_hash String,\n  block_number UInt64,\n  block_timestamp DateTime,\n  block_timestamp_month Date,\n  input_count UInt64,\n  output_count UInt64,\n  input_value Float64,\n  output_value Float64,\n  is_coinbase BOOL,\n  fee Float64,\n  inputs Array(Tuple(index UInt64, spent_transaction_hash String, spent_output_index UInt64, script_asm String, script_hex String, sequence UInt64, required_signatures UInt64, type String, addresses Array(String), value Float64)),\n  outputs Array(Tuple(index UInt64, script_asm String, script_hex String, required_signatures UInt64, type String, addresses Array(String), value Float64))\n)\nENGINE = Kafka('localhost:9092', 'transactions', 'bitcoin-group', 'JSONEachRow') settings kafka_thread_per_consumer = 0, kafka_num_consumers = 3;\n"})}),"\n",(0,i.jsxs)(n.p,{children:["We discuss engine settings and performance tuning below. At this point, a simple select on the table ",(0,i.jsx)(n.code,{children:"blocks_queue"})," should read some rows.  Note that this will move the consumer offsets forward, preventing these rows from being re-read without a ",(0,i.jsx)(n.a,{href:"#common-operations",children:"reset"}),". Note the limit and required parameter ",(0,i.jsx)(n.code,{children:"stream_like_engine_allow_direct_select."})]}),"\n",(0,i.jsx)(n.h2,{id:"6-create-the-materialized-view",children:"6. Create the materialized view"}),"\n",(0,i.jsx)(n.p,{children:"The materialized view will connect the two previously created tables, reading data from the Kafka table engine and inserting it into the target merge tree table. We can do a number of data transformations. We will do a simple read and insert. The use of * assumes column names are identical (case sensitive)."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE MATERIALIZED VIEW blocks_mv TO blocks AS\nSELECT *\nFROM blocks_queue;\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE MATERIALIZED VIEW transactions_mv TO transactions AS\nSELECT *\nFROM transactions_queue;\n"})}),"\n",(0,i.jsx)(n.p,{children:"At the point of creation, the materialized view connects to the Kafka engine and commences reading: inserting rows into the target table. This process will continue indefinitely, with subsequent message inserts into Kafka being consumed. Feel free to re-run the insertion script to insert further messages to Kafka."}),"\n",(0,i.jsx)(n.h5,{id:"7-confirm-rows-have-been-inserted",children:"7. Confirm rows have been inserted"}),"\n",(0,i.jsx)(n.p,{children:"Confirm data exists in the target table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"SELECT count() FROM blocks;\nSELECT count() FROM transactions;\n"})}),"\n",(0,i.jsx)(n.h4,{id:"common-operations",children:"Common Operations"}),"\n",(0,i.jsx)(n.h5,{id:"stopping--restarting-message-consumption",children:"Stopping & restarting message consumption"}),"\n",(0,i.jsx)(n.p,{children:"To stop message consumption, you can detach the Kafka engine table:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"DETACH TABLE blocks_queue;\n"})}),"\n",(0,i.jsx)(n.p,{children:"This will not impact the offsets of the consumer group. To restart consumption, and continue from the previous offset, reattach the table."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"ATTACH TABLE blocks_queue;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"adding-kafka-metadata",children:"Adding Kafka Metadata"}),"\n",(0,i.jsx)(n.p,{children:"It can be useful to keep track of the metadata from the original Kafka messages after it's been ingested into ClickHouse. For example, we may want to know how much of a specific topic or partition we have consumed. For this purpose, the Kafka table engine exposes several virtual columns. These can be persisted as columns in our target table by modifying our schema and materialized view's select statement."}),"\n",(0,i.jsx)(n.p,{children:"First, we perform the stop operation described above before adding columns to our target table."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"DETACH TABLE blocks_queue;\n"})}),"\n",(0,i.jsx)(n.p,{children:"Below we add information columns to identify the source topic and the partition from which the row originated."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"ALTER TABLE blocks\n   ADD COLUMN topic String,\n   ADD COLUMN partition UInt64;\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Next, we need to ensure virtual columns are mapped as required.\nVirtual columns are prefixed with ",(0,i.jsx)(n.code,{children:"_"}),".\nA complete listing of virtual columns can be found ",(0,i.jsx)(n.a,{href:"https://clickhouse.com/docs/engines/table-engines/integrations/kafka#virtual-columns",children:"here"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"To update our table with the virtual columns, we'll need to drop the materialized view, re-attach the Kafka engine table, and re-create the materialized view."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"DROP VIEW blocks_mv;\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"ATTACH TABLE blocks_queue;\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE MATERIALIZED VIEW blocks_mv TO github AS\nSELECT *, _topic as topic, _partition as partition\nFROM blocks_queue;\n"})}),"\n",(0,i.jsx)(n.p,{children:"Newly consumed rows should have the metadata."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"SELECT hash, number, topic, partition\nFROM blocks\nLIMIT 10;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"modify-kafka-engine-settings",children:"Modify Kafka Engine Settings"}),"\n",(0,i.jsx)(n.p,{children:"We recommend dropping the Kafka engine table and recreating it with the new settings. The materialized view does not need to be modified during this process - message consumption will resume once the Kafka engine table is recreated."}),"\n",(0,i.jsx)(n.h2,{id:"debugging-issues",children:"Debugging Issues"}),"\n",(0,i.jsxs)(n.p,{children:["Errors such as authentication issues are not reported in responses to Kafka engine DDL. For diagnosing issues, we recommend using the main ClickHouse log file clickhouse-server.err.log. Further trace logging for the underlying Kafka client library ",(0,i.jsx)(n.a,{href:"https://github.com/edenhill/librdkafka",children:"librdkafka"})," can be enabled through configuration."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:"<kafka>\n   <debug>all</debug>\n</kafka>\n"})})]})}function h(e={}){let{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},80980:function(e,n,t){t.d(n,{Z:()=>o,a:()=>r});var a=t(67294);let i={},s=a.createContext(i);function r(e){let n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);