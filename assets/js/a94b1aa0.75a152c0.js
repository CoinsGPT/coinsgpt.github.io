"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([["79388"],{57234:function(e,n,t){t.d(n,{Z:()=>s});let s=t.p+"assets/images/bitcoin_data_pipeline-083bc34cccd93411924337896b88770f.png"},94057:function(e,n,t){t.d(n,{Z:()=>s});let s=t.p+"assets/images/clickhouse-schema-evolution-8ce662ea09a5f64424f9178d22d19418.png"},63914:function(e,n,t){t.r(n),t.d(n,{frontMatter:()=>r,default:()=>h,contentTitle:()=>o,assets:()=>l,toc:()=>c,metadata:()=>s});var s=JSON.parse('{"id":"bitcoin/clickhouse-schema-evolution","title":"05 ClickHouse Schema Evolution","description":"To add new attributes (previousblockhash, transactions, and nTx) to a ClickHouse table using the Kafka engine\u2014assuming your existing table already receives streaming data from Kafka\u2014you must follow a safe schema evolution process due to ClickHouse\'s strict handling of schemas and its decoupled ingestion setup. Here\'s a professional, step-by-step guide as if taught by a professor of ClickHouse + Kafka streaming integration.","source":"@site/docs/bitcoin/clickhouse-schema-evolution.md","sourceDirName":"bitcoin","slug":"/bitcoin/clickhouse-schema-evolution","permalink":"/docs/next/bitcoin/clickhouse-schema-evolution","draft":false,"unlisted":false,"editUrl":"https://github.com/coinsgpt/coinsgpt.github.io/edit/main/website/docs/bitcoin/clickhouse-schema-evolution.md","tags":[],"version":"current","lastUpdatedBy":"Gitcoins","lastUpdatedAt":1752075035000,"frontMatter":{}}'),i=t(85893),a=t(80980);let r={},o="05 ClickHouse Schema Evolution",l={},c=[{value:"Kafka 1: Stop the Kafka Producer",id:"kafka-1-stop-the-kafka-producer",level:2},{value:"Kafka 2: Add new Attributes in Producer(ETL)",id:"kafka-2-add-new-attributes-in-produceretl",level:2},{value:"Blocks 3: Create new Kafka Engine Table",id:"blocks-3-create-new-kafka-engine-table",level:2},{value:"Blocks 4: Create new MergeTree Table",id:"blocks-4-create-new-mergetree-table",level:2},{value:"Blocks 5: Recreate the Materialized View",id:"blocks-5-recreate-the-materialized-view",level:2},{value:"Blocks 5: Create Materialized View",id:"blocks-5-create-materialized-view",level:2},{value:"Tnx 1: Fat row include Array",id:"tnx-1-fat-row-include-array",level:2},{value:"Step 6: Resume the Kafka Producer",id:"step-6-resume-the-kafka-producer",level:2},{value:"Step 7: Validate End-to-End",id:"step-7-validate-end-to-end",level:2}];function d(e){let n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,a.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"05-clickhouse-schema-evolution",children:"05 ClickHouse Schema Evolution"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:t(57234).Z+"",width:"1536",height:"1024"})}),"\n",(0,i.jsx)(n.p,{children:"To add new attributes (previousblockhash, transactions, and nTx) to a ClickHouse table using the Kafka engine\u2014assuming your existing table already receives streaming data from Kafka\u2014you must follow a safe schema evolution process due to ClickHouse's strict handling of schemas and its decoupled ingestion setup. Here's a professional, step-by-step guide as if taught by a professor of ClickHouse + Kafka streaming integration."}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Layer"}),(0,i.jsx)(n.th,{children:"Action"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Kafka Producer"}),(0,i.jsx)(n.td,{children:"Add the new fields to emitted JSON"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Kafka Engine Table"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"ALTER TABLE blocks_queue ADD COLUMN"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Materialized View"}),(0,i.jsxs)(n.td,{children:["Drop and recreate ",(0,i.jsx)(n.code,{children:"blocks_mv"})," with new SELECT clause"]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Destination Table"}),(0,i.jsxs)(n.td,{children:["Add new fields using ",(0,i.jsx)(n.code,{children:"ALTER TABLE blocks ADD COLUMN"})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Verification"}),(0,i.jsxs)(n.td,{children:["Use ",(0,i.jsx)(n.code,{children:"SELECT"})," queries to validate parsing"]})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:t(94057).Z+"",width:"1536",height:"1024"})}),"\n",(0,i.jsxs)(n.p,{children:["In production systems, ",(0,i.jsx)(n.strong,{children:"graceful schema evolution"})," means:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Pausing ingestion"})}),"\n",(0,i.jsx)(n.li,{children:"Updating schema"}),"\n",(0,i.jsx)(n.li,{children:"Restarting streaming with minimal data loss or duplication"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Below is a ",(0,i.jsx)(n.strong,{children:"professor-level, zero-downtime-friendly guide"})," to ",(0,i.jsx)(n.strong,{children:"safely modify a Kafka-to-ClickHouse pipeline"})," using your components:"]}),"\n",(0,i.jsx)(n.h2,{id:"kafka-1-stop-the-kafka-producer",children:"Kafka 1: Stop the Kafka Producer"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["Pause the source application producing data to the ",(0,i.jsx)(n.code,{children:"blocks"})," Kafka topic to prevent in-flight writes while you upgrade schema."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Stop the producer app or its Kafka client (Ctrl + C)\npython3 bitcoinetl.py stream -p http://bitcoin:password@localhost:8332 --output kafka/localhost:9092 --period-seconds 0 -b 100 -B 1000 --log-file log --enrich True -l last_synced_block.txt\n\nDETACH blocks_queue\n"})}),"\n",(0,i.jsx)(n.h2,{id:"kafka-2-add-new-attributes-in-produceretl",children:"Kafka 2: Add new Attributes in Producer(ETL)"}),"\n",(0,i.jsxs)(n.p,{children:["Add previous_block_hash, difficulty and nTx. ",(0,i.jsx)(n.a,{href:"https://github.com/CoinsGPT/bitcoin-etl/commit/3435d582d3c54f5b1db134f9e87f678635a22804",children:"This commit mainly focuses on adding block-only streaming support, improving block metadata handling, and enhancing Kafka/ClickHouse integration and documentation."})]}),"\n",(0,i.jsx)(n.h2,{id:"blocks-3-create-new-kafka-engine-table",children:"Blocks 3: Create new Kafka Engine Table"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE blocks_queue_v1\n(\n  hash String,\n  size UInt64,\n  stripped_size UInt64,\n  weight UInt64,\n  number UInt64,\n  version UInt64,\n  merkle_root String,\n  timestamp DateTime,\n  nonce String,\n  bits String,\n  coinbase_param String,\n  transaction_count UInt64,\n  previous_block_hash String,\n  difficulty Float64,\n  nTx UInt64,\n  transactions Array(String)\n)\nENGINE = Kafka('localhost:9092', 'blocks', 'bitcoin-group', 'JSONEachRow') settings kafka_thread_per_consumer = 0, kafka_num_consumers = 3;\n"})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["If unsure all messages will have these fields, use ",(0,i.jsx)(n.code,{children:"Nullable(...)"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"blocks-4-create-new-mergetree-table",children:"Blocks 4: Create new MergeTree Table"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE blocks_fat\n(\n  hash String,\n  size UInt64,\n  stripped_size UInt64,\n  weight UInt64,\n  number UInt64,\n  version UInt64,\n  merkle_root String,\n  timestamp DateTime,\n  timestamp_month Date,\n  nonce String,\n  bits String,\n  coinbase_param String,\n  transaction_count UInt64,\n  previous_block_hash String,\n  difficulty Float64,\n  nTx UInt64,\n  transactions Array(String)\n)\nENGINE = ReplacingMergeTree()\nPRIMARY KEY (hash)\nPARTITION BY toYYYYMM(timestamp_month)\nORDER BY hash;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"blocks-5-recreate-the-materialized-view",children:"Blocks 5: Recreate the Materialized View"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE MATERIALIZED VIEW blocks_mv_v1 TO blocks_fat\nAS\nSELECT\n    *,\n    toStartOfMonth(timestamp) AS timestamp_month\nFROM blocks_queue_v1;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"blocks-5-create-materialized-view",children:"Blocks 5: Create Materialized View"}),"\n",(0,i.jsx)(n.p,{children:"CREATE TABLE blocks\n(\nhash String,\nsize UInt64,\nstripped_size UInt64,\nweight UInt64,\nnumber UInt64,\nversion UInt64,\nmerkle_root String,\ntimestamp DateTime,\ntimestamp_month Date,\nnonce String,\nbits String,\ncoinbase_param String,\ntransaction_count UInt64,\nprevious_block_hash String,\ndifficulty Float64,\nnTx UInt64,\n)\nENGINE = MergeTree()\nPRIMARY KEY (hash)\nPARTITION BY toYYYYMM(timestamp_month)\nORDER BY hash;"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\n\n```sql\nCREATE MATERIALIZED VIEW mv_blocks\nTO blocks\nAS\nSELECT\n  hash,\n  size,\n  stripped_size,\n  weight,\n  number,\n  version,\n  merkle_root,\n  timestamp,\n  timestamp_month,\n  nonce,\n  bits,\n  coinbase_param,\n  transaction_count,\n  previous_block_hash,\n  difficulty,\n  nTx \nFROM blocks_fat;\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"SET max_partitions_per_insert_block = 500;\n"})}),"\n",(0,i.jsx)(n.p,{children:"Manual Backfill to blocks"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"INSERT INTO blocks\nSELECT\n  hash,\n  size,\n  stripped_size,\n  weight,\n  number,\n  version,\n  merkle_root,\n  timestamp,\n  timestamp_month,\n  nonce,\n  bits,\n  coinbase_param,\n  transaction_count,\n  previous_block_hash,\n  difficulty,\n  nTx\nFROM blocks_fat;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"tnx-1-fat-row-include-array",children:"Tnx 1: Fat row include Array"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE transactions_fat\n(\n  hash String,\n  size UInt64,\n  virtual_size UInt64,\n  version UInt64,\n  lock_time UInt64,\n  block_hash String,\n  block_number UInt64,\n  block_timestamp DateTime,\n  block_timestamp_month Date,\n  input_count UInt64,\n  output_count UInt64,\n  input_value Float64,\n  output_value Float64,\n  is_coinbase BOOL,\n  fee Float64,\n  inputs Array(Tuple(index UInt64, spent_transaction_hash String, spent_output_index UInt64, script_asm String, script_hex String, sequence UInt64, required_signatures UInt64, type String, addresses Array(String), value Float64)),\n  outputs Array(Tuple(index UInt64, script_asm String, script_hex String, required_signatures UInt64, type String, addresses Array(String), value Float64))\n)\nENGINE = ReplacingMergeTree()\nPRIMARY KEY (hash)\nPARTITION BY toYYYYMM(block_timestamp_month)\nORDER BY (hash);\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"INSERT INTO transactions_fat (\n  hash, size, virtual_size, version, lock_time,\n  block_hash, block_number,\n  input_count, output_count,\n  input_value, output_value,\n  is_coinbase, fee,\n  inputs, outputs,\n  block_timestamp, block_timestamp_month\n)\nSELECT\n  hash, size, virtual_size, version, lock_time,\n  block_hash, block_number,\n  input_count, output_count,\n  input_value, output_value,\n  is_coinbase, fee,\n  inputs, outputs,\n  b.timestamp AS block_timestamp,\n  b.timestamp_month AS block_timestamp_month\nFROM transactions_v1\nINNER JOIN blocks AS b ON block_hash = b.hash;\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"SELECT\n    partition,\n    count()\nFROM system.parts\nWHERE table = 'transactions_fat'\n  AND active\nGROUP BY partition\nORDER BY partition;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"step-6-resume-the-kafka-producer",children:"Step 6: Resume the Kafka Producer"}),"\n",(0,i.jsxs)(n.p,{children:["Now that the pipeline is upgraded, ",(0,i.jsx)(n.strong,{children:"restart"})," the producer:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Restart your Kafka producer\npython3 bitcoinetl.py stream_block -p http://bitcoin:passw0rd@localhost:8332 --output kafka/localhost:9092 --period-seconds 0 -b 100 -B 500 --enrich false --start-block 0\n"})}),"\n",(0,i.jsx)(n.h2,{id:"step-7-validate-end-to-end",children:"Step 7: Validate End-to-End"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"SELECT\n    hash,\n    previousblockhash,\n    nTx,\n    length(transactions) AS tx_count\nFROM blocks\nORDER BY height DESC\nLIMIT 10;\n"})}),"\n",(0,i.jsx)(n.p,{children:"Ensure everything flows correctly."})]})}function h(e={}){let{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},80980:function(e,n,t){t.d(n,{Z:()=>o,a:()=>r});var s=t(67294);let i={},a=s.createContext(i);function r(e){let n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);