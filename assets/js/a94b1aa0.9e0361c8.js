"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([["79388"],{57234:function(e,n,t){t.d(n,{Z:()=>s});let s=t.p+"assets/images/bitcoin_data_pipeline-083bc34cccd93411924337896b88770f.png"},94057:function(e,n,t){t.d(n,{Z:()=>s});let s=t.p+"assets/images/clickhouse-schema-evolution-8ce662ea09a5f64424f9178d22d19418.png"},63914:function(e,n,t){t.r(n),t.d(n,{frontMatter:()=>o,default:()=>h,contentTitle:()=>r,assets:()=>c,toc:()=>l,metadata:()=>s});var s=JSON.parse('{"id":"bitcoin/clickhouse-schema-evolution","title":"05 ClickHouse Schema Evolution","description":"To add new attributes (previousblockhash, transactions, and nTx) to a ClickHouse table using the Kafka engine\u2014assuming your existing table already receives streaming data from Kafka\u2014you must follow a safe schema evolution process due to ClickHouse\'s strict handling of schemas and its decoupled ingestion setup. Here\'s a professional, step-by-step guide as if taught by a professor of ClickHouse + Kafka streaming integration.","source":"@site/docs/bitcoin/clickhouse-schema-evolution.md","sourceDirName":"bitcoin","slug":"/bitcoin/clickhouse-schema-evolution","permalink":"/docs/next/bitcoin/clickhouse-schema-evolution","draft":false,"unlisted":false,"editUrl":"https://github.com/coinsgpt/coinsgpt.github.io/edit/main/website/docs/bitcoin/clickhouse-schema-evolution.md","tags":[],"version":"current","lastUpdatedBy":"thebestornothing","lastUpdatedAt":1748582758000,"frontMatter":{},"sidebar":"bitcoin","previous":{"title":"04 ClickHouse Schema","permalink":"/docs/next/bitcoin/clickhouse-schema"},"next":{"title":"05 Kafka Table Engine","permalink":"/docs/next/bitcoin/kafka-table-engine"}}'),i=t(85893),a=t(80980);let o={},r="05 ClickHouse Schema Evolution",c={},l=[{value:"Step 1: Stop the Kafka Producer",id:"step-1-stop-the-kafka-producer",level:2},{value:"Step 2: Add new Attributes in Producer(ETL)",id:"step-2-add-new-attributes-in-produceretl",level:2},{value:"Step 3: Create new Kafka Engine Table",id:"step-3-create-new-kafka-engine-table",level:2},{value:"Step 4: Create new MergeTree Table",id:"step-4-create-new-mergetree-table",level:2},{value:"Step 5: Recreate the Materialized View",id:"step-5-recreate-the-materialized-view",level:2},{value:"Step 6: Resume the Kafka Producer",id:"step-6-resume-the-kafka-producer",level:2},{value:"Step 7: Validate End-to-End",id:"step-7-validate-end-to-end",level:2}];function d(e){let n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,a.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"05-clickhouse-schema-evolution",children:"05 ClickHouse Schema Evolution"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:t(57234).Z+"",width:"1536",height:"1024"})}),"\n",(0,i.jsx)(n.p,{children:"To add new attributes (previousblockhash, transactions, and nTx) to a ClickHouse table using the Kafka engine\u2014assuming your existing table already receives streaming data from Kafka\u2014you must follow a safe schema evolution process due to ClickHouse's strict handling of schemas and its decoupled ingestion setup. Here's a professional, step-by-step guide as if taught by a professor of ClickHouse + Kafka streaming integration."}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Layer"}),(0,i.jsx)(n.th,{children:"Action"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Kafka Producer"}),(0,i.jsx)(n.td,{children:"Add the new fields to emitted JSON"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Kafka Engine Table"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"ALTER TABLE blocks_queue ADD COLUMN"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Materialized View"}),(0,i.jsxs)(n.td,{children:["Drop and recreate ",(0,i.jsx)(n.code,{children:"blocks_mv"})," with new SELECT clause"]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Destination Table"}),(0,i.jsxs)(n.td,{children:["Add new fields using ",(0,i.jsx)(n.code,{children:"ALTER TABLE blocks ADD COLUMN"})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Verification"}),(0,i.jsxs)(n.td,{children:["Use ",(0,i.jsx)(n.code,{children:"SELECT"})," queries to validate parsing"]})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:t(94057).Z+"",width:"1536",height:"1024"})}),"\n",(0,i.jsxs)(n.p,{children:["In production systems, ",(0,i.jsx)(n.strong,{children:"graceful schema evolution"})," means:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Pausing ingestion"})}),"\n",(0,i.jsx)(n.li,{children:"Updating schema"}),"\n",(0,i.jsx)(n.li,{children:"Restarting streaming with minimal data loss or duplication"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Below is a ",(0,i.jsx)(n.strong,{children:"professor-level, zero-downtime-friendly guide"})," to ",(0,i.jsx)(n.strong,{children:"safely modify a Kafka-to-ClickHouse pipeline"})," using your components:"]}),"\n",(0,i.jsx)(n.h2,{id:"step-1-stop-the-kafka-producer",children:"Step 1: Stop the Kafka Producer"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["Pause the source application producing data to the ",(0,i.jsx)(n.code,{children:"blocks"})," Kafka topic to prevent in-flight writes while you upgrade schema."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Stop the producer app or its Kafka client (Ctrl + C)\npython3 bitcoinetl.py stream -p http://bitcoin:password@localhost:8332 --output kafka/localhost:9092 --period-seconds 0 -b 100 -B 1000 --log-file log --enrich True -l last_synced_block.txt\n\nDETACH blocks_queue\n"})}),"\n",(0,i.jsx)(n.h2,{id:"step-2-add-new-attributes-in-produceretl",children:"Step 2: Add new Attributes in Producer(ETL)"}),"\n",(0,i.jsxs)(n.p,{children:["Add previous_block_hash, difficulty and nTx. ",(0,i.jsx)(n.a,{href:"https://github.com/CoinsGPT/bitcoin-etl/commit/3435d582d3c54f5b1db134f9e87f678635a22804",children:"This commit mainly focuses on adding block-only streaming support, improving block metadata handling, and enhancing Kafka/ClickHouse integration and documentation."})]}),"\n",(0,i.jsx)(n.h2,{id:"step-3-create-new-kafka-engine-table",children:"Step 3: Create new Kafka Engine Table"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE blocks_queue_v1\n(\n  hash String,\n  size UInt64,\n  stripped_size UInt64,\n  weight UInt64,\n  number UInt64,\n  version UInt64,\n  merkle_root String,\n  timestamp DateTime,\n  nonce String,\n  bits String,\n  coinbase_param String,\n  transaction_count UInt64,\n  previous_block_hash String,\n  difficulty Float64,\n  nTx UInt64,\n  transactions Array(String)\n)\nENGINE = Kafka('localhost:9092', 'blocks', 'bitcoin-group', 'JSONEachRow') settings kafka_thread_per_consumer = 0, kafka_num_consumers = 3;\n"})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["If unsure all messages will have these fields, use ",(0,i.jsx)(n.code,{children:"Nullable(...)"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"step-4-create-new-mergetree-table",children:"Step 4: Create new MergeTree Table"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE blocks_v1\n(\n  hash String,\n  size UInt64,\n  stripped_size UInt64,\n  weight UInt64,\n  number UInt64,\n  version UInt64,\n  merkle_root String,\n  timestamp DateTime,\n  timestamp_month Date,\n  nonce String,\n  bits String,\n  coinbase_param String,\n  transaction_count UInt64,\n  previous_block_hash String,\n  difficulty Float64,\n  nTx UInt64,\n  transactions Array(String)\n)\nENGINE = ReplacingMergeTree()\nPRIMARY KEY (hash)\nPARTITION BY toYYYYMM(timestamp_month)\nORDER BY hash;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"step-5-recreate-the-materialized-view",children:"Step 5: Recreate the Materialized View"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"CREATE MATERIALIZED VIEW blocks_mv_v1 TO blocks_v1\nAS\nSELECT\n    *,\n    toStartOfMonth(timestamp) AS timestamp_month\nFROM blocks_queue_v1;\n"})}),"\n",(0,i.jsx)(n.h2,{id:"step-6-resume-the-kafka-producer",children:"Step 6: Resume the Kafka Producer"}),"\n",(0,i.jsxs)(n.p,{children:["Now that the pipeline is upgraded, ",(0,i.jsx)(n.strong,{children:"restart"})," the producer:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Restart your Kafka producer\npython3 bitcoinetl.py stream_block -p http://bitcoin:passw0rd@localhost:8332 --output kafka/localhost:9092 --period-seconds 0 -b 100 -B 500 --enrich false --start-block 0\n"})}),"\n",(0,i.jsx)(n.h2,{id:"step-7-validate-end-to-end",children:"Step 7: Validate End-to-End"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"SELECT\n    hash,\n    previousblockhash,\n    nTx,\n    length(transactions) AS tx_count\nFROM blocks\nORDER BY height DESC\nLIMIT 10;\n"})}),"\n",(0,i.jsx)(n.p,{children:"Ensure everything flows correctly."})]})}function h(e={}){let{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},80980:function(e,n,t){t.d(n,{Z:()=>r,a:()=>o});var s=t(67294);let i={},a=s.createContext(i);function o(e){let n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);